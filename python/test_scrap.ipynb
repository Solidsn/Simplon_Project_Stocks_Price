{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36964bitabfbc18683bd401b9771b6321be56e6d",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#je fais mes import\n",
    "from datetime import datetime\n",
    "import bs4\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import lxml.html\n",
    "import time\n",
    "import random\n",
    "from random import randint\n",
    "import logging\n",
    "import collections\n",
    "from time import gmtime, strftime\n",
    "import html5lib\n",
    "import re\n",
    "import os\n",
    "#date=strftime(\"%Y-%m-%d\")\n",
    "#avec mysql.connector\n",
    "#Nettoyage\n",
    "def clean_nbs(f):\n",
    "    f = float(re.sub('\\xa0','',f).replace(\",\",\".\"))\n",
    "    return f\n",
    "def Indice() :\n",
    "    import mysql.connector\n",
    "    \n",
    "    # mydb = mysql.connector.connect(\n",
    "    #   host=\"localhost\",\n",
    "    #   user=\"root\",\n",
    "    #   passwd=\"1996A1996a/\",\n",
    "    #   database=\"BOURSE\"\n",
    "    # )\n",
    "    # print(mydb)\n",
    "    \n",
    "    #je m'occupe de chaque action de la bourse \n",
    "    url='https://investir.lesechos.fr/actions/cotations/cours-az-cac-40.html'\n",
    "    r = requests.get(url)\n",
    "    print(url, r.status_code)\n",
    "    soup_ACT = BeautifulSoup(r.content,'html.parser')\n",
    "    #Je prend tt les href ACTION\n",
    "    href_action = []\n",
    "    for elem in soup_ACT.find_all('a'):\n",
    "        if \"cours/action\" in elem.get(\"href\") and \"investir.lesechos\" not in elem.get(\"href\") :\n",
    "            href_action.append(\"https://investir.lesechos.fr\"+elem.get(\"href\"))\n",
    "    len(href_action)\n",
    "    #POUR CHAQUE ACTION JE PREND LES ELEMENT\n",
    "    for elem in href_action:\n",
    "        #time.sleep(random.uniform(1.0, 1.5))\n",
    "        url= elem\n",
    "        r = requests.get(url)\n",
    "        print(url, r.status_code)#200 == Good\n",
    "        soup_hist = BeautifulSoup(r.content,'html.parser')\n",
    "\n",
    "        if r.status_code != 200:\n",
    "            print(f\"probl√®me l'url suivante{r.status_code,url}\")\n",
    "            continue\n",
    "\n",
    "        Dernier_A = []\n",
    "        Var_A = []\n",
    "        Haut_A = []\n",
    "        Bas_A = []\n",
    "        Open_A = []\n",
    "        Volume_A = []\n",
    "        ISIN = []\n",
    "\n",
    "        for elem in soup_hist.find_all('div',attrs={\"class\":\"val-cours\"}):\n",
    "            Val = (re.findall(\"\\d+.\",elem.span.text.strip()))\n",
    "            join = \"\".join(Val)\n",
    "            elemClean = clean_nbs(join)\n",
    "            Dernier_A.append(elemClean)\n",
    "\n",
    "        for elem in soup_hist.find_all('div',attrs={\"data-field\":\"variationHeaderFiche\"}):\n",
    "            elV = clean_nbs(elem.text.strip().replace(\"%\",\"\"))\n",
    "            Var_A.append(elV)\n",
    "\n",
    "        for elem in soup_hist.find_all('span',attrs={\"data-field\":\"high\"}):\n",
    "            Val = (re.findall(\"\\d+.\",elem.text.strip()))\n",
    "            join = \"\".join(Val)\n",
    "            elemClean = clean_nbs(join)\n",
    "            Haut_A.append(elemClean)\n",
    "\n",
    "        for elem in soup_hist.find_all('span',attrs={\"data-field\":\"low\"}):\n",
    "            Val = (re.findall(\"\\d+.\",elem.text.strip()))\n",
    "            join = \"\".join(Val)\n",
    "            elemClean = clean_nbs(join)\n",
    "            Bas_A.append(elemClean)\n",
    "\n",
    "        for elem in soup_hist.find_all('span',attrs={\"data-field\":\"open\"}):\n",
    "            Val = (re.findall(\"\\d+.\",elem.text.strip()))\n",
    "            join = \"\".join(Val)\n",
    "            elemClean = clean_nbs(join)\n",
    "            Open_A.append(elemClean)\n",
    "\n",
    "        for elem in soup_hist.find_all('span',attrs={\"data-field\":\"volume\"}):\n",
    "            Val = (re.findall(\"\\d+.\",elem.text.strip()))\n",
    "            join = \"\".join(Val)\n",
    "            try :\n",
    "                elemClean = clean_nbs(join)\n",
    "                Volume_A.append(elemClean)\n",
    "                print(type(Volume_A))\n",
    "            except :\n",
    "                #elemClean = float(re.sub('\\xa0','',join).replace(\",\",\"\"))\n",
    "                #Volume_A.append(elemClean)\n",
    "                Volume_A = float(0)\n",
    "                print(type(Volume_A))\n",
    "\n",
    "        for elem in soup_hist.find_all('div',attrs={\"class\":\"info-valeur\"}):\n",
    "            ISIN.append(re.findall(\"(\\w{2}\\d+)\", elem.h2.text.strip()))\n",
    "            ISIN = ISIN[0]\n",
    "            ISIN = \"\".join(ISIN)\n",
    "\n",
    "        print(Volume_A) \n",
    "        #try :  \n",
    "        df_ACT = pd.DataFrame({\"Valeur\" :(Dernier_A),\"Variation\" :(Var_A),\n",
    "            \"+Haut\" :(Haut_A[0]),\"+Petit\":(Bas_A[0]) ,\"Open\":(Open_A[0]) ,\"Volume_A\":(Volume_A), \"ISIN\":(ISIN)})\n",
    "    return df_ACT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ]
}